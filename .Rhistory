labs(title = paste0("Sentiment for ", i),
x = "Year",
y = "Value") +
theme(plot.title = element_text(size = 14, face = "bold")) +
#theme(legend.position = "none") +
scale_fill_manual(values = cols8)
# Create the positive/negative proportion plot
plot2 <- ggplot(pos_neg_continent_data, aes(x = year, y = value, fill = variable)) +
geom_stream(type = "proportional") +
#geom_stream_label(aes(label = variable)) +
theme_minimal() +
labs(title = paste0("Aggregated for ", i),
x = "Year",
y = "Value") +
theme(plot.title = element_text(size = 14, face = "bold")) +
#theme(legend.position = "none") +
scale_fill_manual(values = cols2)
# Arrange the plots side by side
combined_plots <- grid.arrange(plot1, plot2, ncol = 2)
# Save the combined plots to files
path <- paste0("results/analysis/r3/", i, "_plots.pdf")
#path2 <- paste0("results/analysis/r2/", i, "_plots.eps")
ggsave(path, combined_plots, width = 8, height = 2.5)
#ggsave(path2, combined_plots, width = 7.5, height = 3)
print(grid.arrange(grid.arrange(plot1, plot2, ncol = 2)))
# Arrange the plots side by side
print(grid.arrange(grid.arrange(plot1, plot2, ncol = 2)))
# Save the combined plots to files
path <- paste0("results/analysis/r3/", i, "_plots.pdf")
#path2 <- paste0("results/analysis/r2/", i, "_plots.eps")
ggsave(path, width = 8, height = 2.5)
# Arrange the plots side by side
combined_plots <- grid.arrange(plot1, plot2, ncol = 2)
arrangeGrob(plot1, plot2, ncol = 2)
#path2 <- paste0("results/analysis/r2/", i, "_plots.eps")
ggsave(path, g, width = 8, height = 2.5)
g <- arrangeGrob(plot1, plot2, ncol = 2)
# Save the combined plots to files
path <- paste0("results/analysis/r3/", i, "_plots.pdf")
#path2 <- paste0("results/analysis/r2/", i, "_plots.eps")
ggsave(path, g, width = 8, height = 2.5)
#path2 <- paste0("results/analysis/r2/", i, "_plots.eps")
ggsave(path, plot = g, width = 8, height = 2.5)
paste0("results/analysis/r3/", i, "_plots.pdf")
rm(list = ls())
gc()
cat("\014")
## define the working directory
setwd("D:/GDrive/Faculdade/Mestrado/3.WS 2023/Lecture - Programming for Economists/Project")
getwd()
# relevant packages
packages <- c("tidyverse", "tm", "slam", "rvest", "xml2", "stringdist", "countrycode", "quanteda",
"SnowballC", "ggformula", "ggpubr", "scales", "readxl" , "stopwords", "wordcloud",
"syuzhet", "tidytext", "gridExtra", "ggstream")
# check if installed, otherwise, install, omit warnings
for(p in packages){
if(!require(p, character.only = TRUE)){
suppressMessages(suppressWarnings(install.packages(p)))
library(p, character.only = TRUE)
}
}
## read the raw text to do wordcloud, path C:\Users\crist\Downloads\chat_w_gabi.txt
mydata <- readlines("data/raw_text/chat_w_gabi.txt")
library(tm)
## read the raw text to do wordcloud, path C:\Users\crist\Downloads\chat_w_gabi.txt
mydata <- readlines("data/raw_text/chat_w_gabi.txt")
## read the raw text to do wordcloud, path C:\Users\crist\Downloads\chat_w_gabi.txt
mydata <- readline("data/raw_text/chat_w_gabi.txt")
mydata <- readline("data/raw_text/chat_w_gabi.txt")
mydata <- readline("data/raw_text/chat_w_gabi.txt")
mydata <- readline("data/raw_text/chat_w_gabi.txt")
mydata <- readLines("data/raw_text/chat_w_gabi.txt")
mydata <- readLines("C:\\Users\\crist\\Downloads\\chat_w_gabi.txt")
mydata
mydata <- readLines("C:\\Users\\crist\\Downloads\\chat_w_gabi.txt", encoding="UTF-8")
## let's put all words to lowercase
mydata <- tolower(mydata)
## let's remove numbers
mydata <- gsub("[[:digit:]]", " ", mydata)
## let's remove punctuation
mydata <- gsub("[[:punct:]]", " ", mydata)
## print original encoding
Encoding(mydata)
## let's remove accents, from uft8 to ascii
mydata <- iconv(mydata, "UTF-8", "ASCII", sub="byte")
# print sample of text
mydata[1:10]
mydata <- readLines("C:\\Users\\crist\\Downloads\\chat_w_gabi.txt", encoding="UTF-8")
## let's put all words to lowercase
mydata <- tolower(mydata)
## let's remove numbers
mydata <- gsub("[[:digit:]]", " ", mydata)
## let's remove punctuation
mydata <- gsub("[[:punct:]]", " ", mydata)
## print original encoding
Encoding(mydata)
## let's remove accents, from uft8 to ascii
mydata <- iconv(mydata, "UTF-8", "ASCII//TRANSLIT", sub="byte")
mydata[1:10]
## let's remove special characters
mydata <- gsub("[^[:alnum:][:space:]]", " ", mydata)
mydata <- gsub(paste(list, collapse = "|"), " ", mydata)
list <- c("gui", "guilherme", "schultz", "lindenmeyer")
mydata <- gsub(paste(list, collapse = "|"), " ", mydata)
# print sample
sample(mydata, 10)
## remove gabi's name: gabi gabrielle gabinha bie flor rosenstengel
list <- c("gabi", "gabrielle", "gabinha", "bie", "flor", "rosenstengel")
mydata <- gsub(paste(list, collapse = "|"), " ", mydata)
# print sample
sample(mydata, 10)
# we drop URLs
mydata <- gsub("(?i)([^[:space:]])+www([^[:space:]])+", " ", mydata)
mydata <- gsub("(?i)http([^[:space:]])+", " ", mydata)
# we drop other web address items
mydata <- gsub("(?i)([^[:space:]])+[.](org|com|de)", " ", mydata)
# we drop document names
mydata <- gsub("(?i)draft-ietf-([^[:space:]])+", " ", mydata)
# we re-format e-mail to email
mydata <- gsub("(?i)e-mail", "email", mydata)
# drop html special characters
mydata <- gsub("(?i)&[a-z]+", " ", mydata)
##### ##### ##### ##### #####
# CREATE CORPUS
corp <- VCorpus(DataframeSource(mydata))
data.frame(mydata)
## convert my data to dataframe with column text
mydata <- data.frame(text = mydata)
mydata
mydata$text
##### ##### ##### ##### #####
# CREATE CORPUS
corp <- VCorpus(DataframeSource(mydata))
mydata <- readLines("C:\\Users\\crist\\Downloads\\chat_w_gabi.txt", encoding="UTF-8")
## let's put all words to lowercase
mydata <- tolower(mydata)
## let's remove numbers
mydata <- gsub("[[:digit:]]", " ", mydata)
## let's remove punctuation
mydata <- gsub("[[:punct:]]", " ", mydata)
## print original encoding
Encoding(mydata)
## let's remove accents, from uft8 to ascii
mydata <- iconv(mydata, "UTF-8", "ASCII//TRANSLIT", sub="byte")
## let's remove special characters
mydata <- gsub("[^[:alnum:][:space:]]", " ", mydata)
## remove my name: gui guilherme lindenmeyer schultz
list <- c("gui", "guilherme", "schultz", "lindenmeyer")
mydata <- gsub(paste(list, collapse = "|"), " ", mydata)
## remove gabi's name: gabi gabrielle gabinha bie flor rosenstengel
list <- c("gabi", "gabrielle", "gabinha", "bie", "flor", "rosenstengel")
mydata <- gsub(paste(list, collapse = "|"), " ", mydata)
# print sample
sample(mydata, 10)
# we drop URLs
mydata <- gsub("(?i)([^[:space:]])+www([^[:space:]])+", " ", mydata)
mydata <- gsub("(?i)http([^[:space:]])+", " ", mydata)
mydata <- gsub("(?i)([^[:space:]])+@([^[:space:]])+", " ", mydata)
# we drop other web address items
mydata <- gsub("(?i)([^[:space:]])+[.](org|com|de)", " ", mydata)
# we drop document names
mydata <- gsub("(?i)draft-ietf-([^[:space:]])+", " ", mydata)
# we re-format e-mail to email
mydata <- gsub("(?i)e-mail", "email", mydata)
# drop html special characters
mydata <- gsub("(?i)&[a-z]+", " ", mydata)
# drop html special characters
mydata <- gsub("(?i)&[a-z]+", " ", mydata)
## convert my data to dataframe with column text and doc_id the rownumber
mydata <- data.frame(text = mydata, doc_id = 1:length(mydata))
##### ##### ##### ##### #####
# CREATE CORPUS
corp <- VCorpus(DataframeSource(mydata))
class(corp)
##### # Problems with UTF-8 coding: http://tm.r-forge.r-project.org/faq.html?
corp <- tm_map(corp, content_transformer(function(x) {iconv(enc2utf8(x), sub = "byte")}))
##### # to lower / #convert all words to lower case
corp <- tm_map(corp, content_transformer(tolower));
##### # remove all HTML tags from string
cleanFun <- function(htmlString) {return(gsub("<.*?>", "", htmlString))}
corp <- tm_map(corp, content_transformer(function(x) {return(gsub("<.*?>", "", x))}))
?stopwords
##### # remove stopwords: small words, likely little to no meaning
stopwords("portuguese")
stopwords()
iconv(stopwords("portuguese"), "UTF-8", "ASCII//TRANSLIT", sub="byte")
removeWords
corp <- tm_map(corp, removeWords, iconv(stopwords("portuguese"), "UTF-8", "ASCII//TRANSLIT", sub="byte")); #remove mystopwords
##### # remove punctuation
corp <- tm_map(corp, content_transformer(function(x) {stringr::str_replace_all(x,"[^[:alnum:]]", " ")}))
##### # remove numbers
corp <- tm_map(corp, removeNumbers); #remove numbers
##### # remove single-character words
corp <- tm_map(corp, removeWords, letters); #remove single-character words
##### # remove roman numerals
corp <- tm_map(corp, removeWords, tolower(as.roman(1:200))); #remove roman numerals i through c
##### # extra white space
corp <- tm_map(corp, stripWhitespace); #strip white spaces
##### # UNI-GRAMS
dtm1 <- DocumentTermMatrix(corp)
dim(dtm1)
tdm1 <- TermDocumentMatrix(corp)
# Inspect the dtm
inspect(dtm1)
# find terms with more than 10000 occurrences
findFreqTerms(dtm1, 1000)
# find terms with more than 10000 occurrences
findFreqTerms(dtm1, 100)
# find associate words (used int he same texts)
findAssocs(dtm1, 'protocol', 0.40)
# Dendogram
# dendrogram say? Terms higher in the plot appear more frequently within the corpus;
# terms grouped near to each other are more frequently found together.
my.df <- as.data.frame(inspect(tdm1))
my.df.scale <- scale(my.df)
d <- dist(my.df.scale,method="euclidean")
fit <- hclust(d, method="ward.D2")
plot(fit)
##### # BI-GRAMS
BigramTokenizer <-
function(x) {unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)}
dtm2 <- DocumentTermMatrix(corp, control = list(tokenize = BigramTokenizer))
dim(dtm2)
# Inspect the dtm
inspect(dtm2)
# find terms with more than 10000 occurrences
findFreqTerms(dtm2, 1000)
##### # WORD COUNTS PER DOCUMENT
mydata.wordcounts <- as.data.frame(apply(dtm1,1,sum))
colnames(mydata.wordcounts) <- "totalWords"
mydata.wordcounts["uniqueWords"] <- apply(dtm1,1,Matrix::nnzero)
## simple summary statistics
summary(mydata.wordcounts)
##### # WORD COUNTS PER UNIGRAM
mydata.termcounts <- as.data.frame(apply(dtm1,2,sum)) # 2: the column
colnames(mydata.termcounts) <- "freq"
##### # TYPE-TOKEN RATIO
# (total number of different words divided by total number of words)
help(textstat_lexdiv)
ttratio <- mydata %>%
tokens() %>%
quanteda.textstats::textstat_lexdiv(measure = c("TTR"))
mydata.wordcounts <- cbind(mydata.wordcounts,ttratio$TTR)
mydata.wordcounts["ttratio2"] <- mydata.wordcounts$uniqueWords/mydata.wordcounts$totalWords
summary(mydata.wordcounts)
mydata.wordcounts
wordcloud(rownames(mydata.termcounts),
freq = mydata.termcounts$freq,
scale=c(2, .75),
min.freq = 50,max.freq=200, max.words = 100,
random.order = F,
colors = brewer.pal(10,"Dark2"))
summary(mydata.wordcounts)
mydata <- readLines("C:\\Users\\crist\\Downloads\\chat_w_gabi.txt", encoding="UTF-8")
## let's put all words to lowercase
mydata <- tolower(mydata)
## let's remove numbers
mydata <- gsub("[[:digit:]]", " ", mydata)
## let's remove punctuation
mydata <- gsub("[[:punct:]]", " ", mydata)
## print original encoding
Encoding(mydata)
## let's remove accents, from uft8 to ascii
mydata <- iconv(mydata, "UTF-8", "ASCII//TRANSLIT", sub="byte")
## let's remove special characters
mydata <- gsub("[^[:alnum:][:space:]]", " ", mydata)
## remove my name: gui guilherme lindenmeyer schultz
list <- c("gui", "guilherme", "schultz", "lindenmeyer")
mydata <- gsub(paste(list, collapse = "|"), " ", mydata)
## remove gabi's name: gabi gabrielle gabinha bie flor rosenstengel
list <- c("gabi", "gabrielle", "gabinha", "bie", "flor", "rosenstengel")
mydata <- gsub(paste(list, collapse = "|"), " ", mydata)
# print sample
sample(mydata, 10)
# we drop URLs
mydata <- gsub("(?i)([^[:space:]])+www([^[:space:]])+", " ", mydata)
mydata <- gsub("(?i)http([^[:space:]])+", " ", mydata)
mydata <- gsub("(?i)([^[:space:]])+@([^[:space:]])+", " ", mydata)
# we drop other web address items
mydata <- gsub("(?i)([^[:space:]])+[.](org|com|de)", " ", mydata)
# we drop document names
mydata <- gsub("(?i)draft-ietf-([^[:space:]])+", " ", mydata)
# we re-format e-mail to email
mydata <- gsub("(?i)e-mail", "email", mydata)
# drop html special characters
mydata <- gsub("(?i)&[a-z]+", " ", mydata)
## convert my data to dataframe with column text and doc_id the rownumber
mydata <- data.frame(text = mydata, doc_id = 1:length(mydata))
##### ##### ##### ##### #####
# CREATE CORPUS
corp <- VCorpus(DataframeSource(mydata))
class(corp)
##### # Problems with UTF-8 coding: http://tm.r-forge.r-project.org/faq.html?
corp <- tm_map(corp, content_transformer(function(x) {iconv(enc2utf8(x), sub = "byte")}))
##### # to lower / #convert all words to lower case
corp <- tm_map(corp, content_transformer(tolower));
##### # remove all HTML tags from string
cleanFun <- function(htmlString) {return(gsub("<.*?>", "", htmlString))}
corp <- tm_map(corp, content_transformer(function(x) {return(gsub("<.*?>", "", x))}))
##### # remove stopwords: small words, likely little to no meaning
corp <- tm_map(corp, removeWords, iconv(stopwords("portuguese"), "UTF-8", "ASCII//TRANSLIT", sub="byte")); #remove mystopwords
corp <- tm_map(corp, removeWords, "omitted"); #remove mystopwords
##### # remove punctuation
corp <- tm_map(corp, content_transformer(function(x) {stringr::str_replace_all(x,"[^[:alnum:]]", " ")}))
##### # remove numbers
corp <- tm_map(corp, removeNumbers); #remove numbers
##### # remove single-character words
corp <- tm_map(corp, removeWords, letters); #remove single-character words
##### # remove roman numerals
corp <- tm_map(corp, removeWords, tolower(as.roman(1:200))); #remove roman numerals i through c
##### # extra white space
corp <- tm_map(corp, stripWhitespace); #strip white spaces
##### # UNI-GRAMS
dtm1 <- DocumentTermMatrix(corp)
dim(dtm1)
tdm1 <- TermDocumentMatrix(corp)
# 5000 documents
# 34451 words
##### # WORD COUNTS PER UNIGRAM
mydata.termcounts <- as.data.frame(apply(dtm1,2,sum)) # 2: the column
wordcloud(rownames(mydata.termcounts),
freq = mydata.termcounts$freq,
scale=c(2, .75),
min.freq = 50,max.freq=200, max.words = 100,
random.order = F,
colors = brewer.pal(10,"Dark2"))
colnames(mydata.termcounts) <- "freq"
wordcloud(rownames(mydata.termcounts),
freq = mydata.termcounts$freq,
scale=c(2, .75),
min.freq = 50,max.freq=200, max.words = 100,
random.order = F,
colors = brewer.pal(10,"Dark2"))
mydata <- readLines("C:\\Users\\crist\\Downloads\\chat_w_gabi.txt", encoding="UTF-8")
mydata2 <- readLines("C:\\Users\\crist\\Downloads\\chat_w_gabi2.txt", encoding="UTF-8")
# merge them by row
mydata <- c(mydata, mydata2)
## let's put all words to lowercase
mydata <- tolower(mydata)
## let's remove numbers
mydata <- gsub("[[:digit:]]", " ", mydata)
## let's remove punctuation
mydata <- gsub("[[:punct:]]", " ", mydata)
## print original encoding
Encoding(mydata)
## let's remove accents, from uft8 to ascii
mydata <- iconv(mydata, "UTF-8", "ASCII//TRANSLIT", sub="byte")
## let's remove special characters
mydata <- gsub("[^[:alnum:][:space:]]", " ", mydata)
## remove my name: gui guilherme lindenmeyer schultz
list <- c("gui", "guilherme", "schultz", "lindenmeyer")
mydata <- gsub(paste(list, collapse = "|"), " ", mydata)
## remove gabi's name: gabi gabrielle gabinha bie flor rosenstengel
list <- c("gabi", "gabrielle", "gabinha", "bie", "flor", "rosenstengel")
mydata <- gsub(paste(list, collapse = "|"), " ", mydata)
# print sample
sample(mydata, 10)
# we drop URLs
mydata <- gsub("(?i)([^[:space:]])+www([^[:space:]])+", " ", mydata)
mydata <- gsub("(?i)http([^[:space:]])+", " ", mydata)
mydata <- gsub("(?i)([^[:space:]])+@([^[:space:]])+", " ", mydata)
# we drop other web address items
mydata <- gsub("(?i)([^[:space:]])+[.](org|com|de)", " ", mydata)
# we drop document names
mydata <- gsub("(?i)draft-ietf-([^[:space:]])+", " ", mydata)
# we re-format e-mail to email
mydata <- gsub("(?i)e-mail", "email", mydata)
# drop html special characters
mydata <- gsub("(?i)&[a-z]+", " ", mydata)
## convert my data to dataframe with column text and doc_id the rownumber
mydata <- data.frame(text = mydata, doc_id = 1:length(mydata))
##### ##### ##### ##### #####
# CREATE CORPUS
corp <- VCorpus(DataframeSource(mydata))
class(corp)
##### # Problems with UTF-8 coding: http://tm.r-forge.r-project.org/faq.html?
corp <- tm_map(corp, content_transformer(function(x) {iconv(enc2utf8(x), sub = "byte")}))
##### # to lower / #convert all words to lower case
corp <- tm_map(corp, content_transformer(tolower));
##### # remove all HTML tags from string
cleanFun <- function(htmlString) {return(gsub("<.*?>", "", htmlString))}
corp <- tm_map(corp, content_transformer(function(x) {return(gsub("<.*?>", "", x))}))
corp <- tm_map(corp, removeWords, iconv(stopwords("portuguese"), "UTF-8", "ASCII//TRANSLIT", sub="byte")); #remove mystopwords
corp <- tm_map(corp, removeWords, "omitted"); #remove mystopwords
##### # remove punctuation
corp <- tm_map(corp, content_transformer(function(x) {stringr::str_replace_all(x,"[^[:alnum:]]", " ")}))
##### # remove numbers
corp <- tm_map(corp, removeNumbers); #remove numbers
##### # remove single-character words
corp <- tm_map(corp, removeWords, letters); #remove single-character words
##### # remove stopwords: small words, likely little to no meaning
## remove my name: gui guilherme lindenmeyer schultz
list1 <- c("gui", "guilherme", "schultz", "lindenmeyer")
list2 <- c("gabi", "gabrielle", "gabinha", "bie", "flor", "rosenstengel")
list3 <- c("omitted", "media", "audio", "image", "video", "file", "location", "contact", "gifs", "sticker", "gif", "sticker", "document", "voice", "note", "omitted")
rm(list = ls())
gc()
cat("\014")
## define the working directory
setwd("D:/GDrive/Faculdade/Mestrado/3.WS 2023/Lecture - Programming for Economists/Project")
getwd()
# relevant packages
packages <- c("tidyverse", "tm", "slam", "rvest", "xml2", "stringdist", "countrycode", "quanteda",
"SnowballC", "ggformula", "ggpubr", "scales", "readxl" , "stopwords", "wordcloud",
"syuzhet", "tidytext", "gridExtra", "ggstream")
# check if installed, otherwise, install, omit warnings
for(p in packages){
if(!require(p, character.only = TRUE)){
suppressMessages(suppressWarnings(install.packages(p)))
library(p, character.only = TRUE)
}
}
## read the raw text to do wordcloud, path C:\Users\crist\Downloads\chat_w_gabi.txt
mydata <- readLines("C:\\Users\\crist\\Downloads\\chat_w_gabi.txt", encoding="UTF-8")
mydata2 <- readLines("C:\\Users\\crist\\Downloads\\chat_w_gabi2.txt", encoding="UTF-8")
# merge them by row
mydata <- c(mydata, mydata2)
## let's put all words to lowercase
mydata <- tolower(mydata)
## let's remove numbers
mydata <- gsub("[[:digit:]]", " ", mydata)
## let's remove punctuation
mydata <- gsub("[[:punct:]]", " ", mydata)
## print original encoding
Encoding(mydata)
## let's remove accents, from uft8 to ascii
mydata <- iconv(mydata, "UTF-8", "ASCII//TRANSLIT", sub="byte")
## let's remove special characters
mydata <- gsub("[^[:alnum:][:space:]]", " ", mydata)
mydata <- gsub(paste(list, collapse = "|"), " ", mydata)
# print sample
sample(mydata, 10)
# we drop URLs
mydata <- gsub("(?i)([^[:space:]])+www([^[:space:]])+", " ", mydata)
mydata <- gsub("(?i)http([^[:space:]])+", " ", mydata)
mydata <- gsub("(?i)([^[:space:]])+@([^[:space:]])+", " ", mydata)
# we drop other web address items
mydata <- gsub("(?i)([^[:space:]])+[.](org|com|de)", " ", mydata)
# we drop document names
mydata <- gsub("(?i)draft-ietf-([^[:space:]])+", " ", mydata)
# we re-format e-mail to email
mydata <- gsub("(?i)e-mail", "email", mydata)
# drop html special characters
mydata <- gsub("(?i)&[a-z]+", " ", mydata)
## convert my data to dataframe with column text and doc_id the rownumber
mydata <- data.frame(text = mydata, doc_id = 1:length(mydata))
##### ##### ##### ##### #####
# CREATE CORPUS
corp <- VCorpus(DataframeSource(mydata))
class(corp)
##### # Problems with UTF-8 coding: http://tm.r-forge.r-project.org/faq.html?
corp <- tm_map(corp, content_transformer(function(x) {iconv(enc2utf8(x), sub = "byte")}))
##### # to lower / #convert all words to lower case
corp <- tm_map(corp, content_transformer(tolower));
##### # remove all HTML tags from string
cleanFun <- function(htmlString) {return(gsub("<.*?>", "", htmlString))}
corp <- tm_map(corp, content_transformer(function(x) {return(gsub("<.*?>", "", x))}))
##### # remove stopwords: small words, likely little to no meaning
## remove my name: gui guilherme lindenmeyer schultz
list1 <- c("gui", "guilherme", "schultz", "lindenmeyer")
list2 <- c("gabi", "gabrielle", "gabinha", "bie", "flor", "rosenstengel")
list3 <- c("omitted", "media", "audio", "image", "video", "file", "location", "contact", "gifs", "sticker", "gif", "sticker", "document", "voice", "note", "omitted")
mywords <- c(list1, list2, list3)
corp <- tm_map(corp, removeWords, iconv(stopwords("portuguese"), "UTF-8", "ASCII//TRANSLIT", sub="byte")); #remove mystopwords
corp <- tm_map(corp, removeWords, mywords); #remove mystopwords
##### # remove punctuation
corp <- tm_map(corp, content_transformer(function(x) {stringr::str_replace_all(x,"[^[:alnum:]]", " ")}))
##### # remove numbers
corp <- tm_map(corp, removeNumbers); #remove numbers
##### # remove single-character words
corp <- tm_map(corp, removeWords, letters); #remove single-character words
##### # remove roman numerals
corp <- tm_map(corp, removeWords, tolower(as.roman(1:200))); #remove roman numerals i through c
##### # extra white space
corp <- tm_map(corp, stripWhitespace); #strip white spaces
##### # UNI-GRAMS
dtm1 <- DocumentTermMatrix(corp)
dim(dtm1)
tdm1 <- TermDocumentMatrix(corp)
##### # WORD COUNTS PER UNIGRAM
mydata.termcounts <- as.data.frame(apply(dtm1,2,sum)) # 2: the column
colnames(mydata.termcounts) <- "freq"
wordcloud(rownames(mydata.termcounts),
freq = mydata.termcounts$freq,
scale=c(2, .75),
min.freq = 50,max.freq=200, max.words = 100,
random.order = F,
colors = brewer.pal(10,"Dark2"))
wordcloud(rownames(mydata.termcounts),
freq = mydata.termcounts$freq,
scale=c(2, .75),
min.freq = 50,max.freq=2000, max.words = 1000,
random.order = F,
colors = brewer.pal(10,"Dark2"))
wordcloud(rownames(mydata.termcounts),
freq = mydata.termcounts$freq,
scale=c(2, .75),
min.freq = 100,max.freq=2000, max.words = 100,
random.order = F,
colors = brewer.pal(10,"Dark2"))
wordcloud(rownames(mydata.termcounts),
freq = mydata.termcounts$freq,
scale=c(2, .75),
min.freq = 1000,max.freq=2000, max.words = 100,
random.order = F,
colors = brewer.pal(10,"Dark2"))
wordcloud(rownames(mydata.termcounts),
freq = mydata.termcounts$freq,
scale=c(2, .75),
min.freq = 500,max.freq=2000, max.words = 100,
random.order = F,
colors = brewer.pal(10,"Dark2"))
